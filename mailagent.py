"""
Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.
SPDX-License-Identifier: MIT-0
"""
import os
import re
import json
import time
import html
import logging
import requests
import gradio as gr
import base64
import uuid
from io import BytesIO
import copy
from dotenv import load_dotenv
load_dotenv()  # load env vars from .env
API_KEY = os.environ.get("API_KEY")

logging.basicConfig(level=logging.INFO)
mcp_base_url = os.environ.get('MCP_BASE_URL')
mcp_command_list = ["uvx", "npx", "node", "python", "docker", "uv"]
COOKIE_NAME = "mcp_chat_user_id"

# ç”¨æˆ·ä¼šè¯ç®¡ç†
def get_user_id(request: gr.Request = None):
    """è·å–æˆ–ç”Ÿæˆç”¨æˆ·ID"""
    user_id = None
    if request:
        cookies = request.cookies
        user_id = cookies.get(COOKIE_NAME)
    
    if not user_id:
        user_id = str(uuid.uuid4())[:8]
    
    return user_id

def get_auth_headers(user_id):
    """æ„å»ºåŒ…å«ç”¨æˆ·èº«ä»½çš„è®¤è¯å¤´"""
    headers = {
        'Authorization': f'Bearer {API_KEY}',
        'X-User-ID': user_id
    }
    return headers

def request_list_models(user_id):
    url = mcp_base_url.rstrip('/') + '/v1/list/models'
    models = []
    try:
        logging.info(f'Requesting models list for user: {user_id}')
        headers = get_auth_headers(user_id)
        logging.info(f'Request headers: {headers}')
        logging.info(f'Request URL: {url}')
        
        # Add timeout to prevent hanging
        response = requests.get(url, headers=headers, timeout=10)
        logging.info(f'Response status code: {response.status_code}')
        
        if response.status_code != 200:
            logging.error(f'Error response: {response.text}')
            # If we can't get models from the API, use fallback models
            return [
                {"model_id": "us.anthropic.claude-3-5-sonnet-20241022-v2:0", "model_name": "Claude 3.5 Sonnet v2"},
                {"model_id": "us.amazon.nova-pro-v1:0", "model_name": "Amazon Nova Pro v1"}
            ]
            
        data = response.json()
        models = data.get('models', [])
        logging.info(f'Retrieved models: {models}')
        
        # If no models returned, use fallback models
        if not models:
            logging.warning("No models returned from API, using fallback models")
            return [
                {"model_id": "us.anthropic.claude-3-5-sonnet-20241022-v2:0", "model_name": "Claude 3.5 Sonnet v2"},
                {"model_id": "us.amazon.nova-pro-v1:0", "model_name": "Amazon Nova Pro v1"}
            ]
            
    except Exception as e:
        logging.error(f'Request list models error: {str(e)}')
        import traceback
        logging.error(traceback.format_exc())
        
        # Return fallback models in case of any exception
        logging.warning("Using fallback models due to exception")
        return [
            {"model_id": "us.anthropic.claude-3-5-sonnet-20241022-v2:0", "model_name": "Claude 3.5 Sonnet v2"},
            {"model_id": "us.amazon.nova-pro-v1:0", "model_name": "Amazon Nova Pro v1"}
        ]
        
    return models

def request_list_mcp_servers(user_id):
    url = mcp_base_url.rstrip('/') + '/v1/list/mcp_server'
    mcp_servers = []
    try:
        response = requests.get(url, headers=get_auth_headers(user_id))
        data = response.json()
        mcp_servers = data.get('servers', [])
    except Exception as e:
        logging.error('request list mcp servers error: %s' % e)
    return mcp_servers

def request_add_mcp_server(user_id, server_id, server_name, command, args=[], env=None, config_json={}):
    url = mcp_base_url.rstrip('/') + '/v1/add/mcp_server'
    status = False
    try:
        payload = {
            "server_id": server_id,
            "server_desc": server_name,
            "command": command,
            "args": args,
            "config_json": config_json
        }
        if env:
            payload["env"] = env
        response = requests.post(url, json=payload, headers=get_auth_headers(user_id))
        data = response.json()
        status = data['errno'] == 0
        msg = data['msg']
    except Exception as e:
        msg = "Add MCP server occurred errors!"
        logging.error('request add mcp servers error: %s' % e)
    return status, msg

def process_stream_response(response):
    """Process streaming response and yield content chunks"""
    for line in response.iter_lines():
        if line:
            line = line.decode('utf-8')
            if line.startswith('data: '):
                data = line[6:]  # Remove 'data: ' prefix
                if data == '[DONE]':
                    break
                try:
                    json_data = json.loads(data)
                    delta = json_data['choices'][0].get('delta', {})
                    if 'role' in delta:
                        continue
                    if 'content' in delta:
                        yield delta['content']
                    
                    message_extras = json_data['choices'][0].get('message_extras', {})
                    if "tool_use" in message_extras:
                        yield f"<tool_use>{message_extras['tool_use']}</tool_use>"

                except json.JSONDecodeError:
                    logging.error(f"Failed to parse JSON: {data}")
                except Exception as e:
                    logging.error(f"Error processing stream: {e}")

def request_chat(user_id, messages, model_id, mcp_server_ids, stream=True, max_tokens=1024, temperature=0.6, extra_params={}):
    url = mcp_base_url.rstrip('/') + '/v1/chat/completions'
    msg, msg_extras = 'something is wrong!', {}
    try:
        payload = {
            'messages': messages,
            'model': model_id,
            'mcp_server_ids': mcp_server_ids,
            'extra_params': extra_params,
            'stream': stream,
            'temperature': temperature,
            'max_tokens': max_tokens
        }
        logging.info(f'ç”¨æˆ· {user_id} è¯·æ±‚payload: %s' % payload)
        
        if stream:
            # æµå¼è¯·æ±‚
            headers = get_auth_headers(user_id)
            headers['Accept'] = 'text/event-stream'  
            response = requests.post(url, json=payload, stream=True, headers=headers)
            
            if response.status_code == 200:
                return response, {}
            else:
                msg = 'An error occurred when calling the Converse operation: The system encountered an unexpected error during processing. Try your request again.'
                logging.error(f'ç”¨æˆ· {user_id} è¯·æ±‚èŠå¤©é”™è¯¯: %d' % response.status_code)
        else:
            # å¸¸è§„è¯·æ±‚
            response = requests.post(url, json=payload, headers=get_auth_headers(user_id))
            data = response.json()
            msg = data['choices'][0]['message']['content']
            msg_extras = data['choices'][0]['message_extras']

    except Exception as e:
        msg = 'An error occurred when calling the Converse operation: The system encountered an unexpected error during processing. Try your request again.'
        logging.error(f'ç”¨æˆ· {user_id} è¯·æ±‚èŠå¤©é”™è¯¯: %s' % e)
    
    logging.info(f'ç”¨æˆ· {user_id} å“åº”æ¶ˆæ¯: %s' % msg)
    return msg, msg_extras

def add_new_mcp_server(user_id, server_name, server_id, server_cmd, server_args, server_env, server_config_json):
    status, msg = True, "The server already been added!"
    config_json = {}
    
    if not server_name:
        status, msg = False, "The server name is empty!"
    
    # å¦‚æœserver_config_jsoné…ç½®ï¼Œåˆ™å·²server_config_jsonä¸ºå‡†
    if server_config_json:
        try:
            config_json = json.loads(server_config_json)
            if not all([isinstance(k, str) for k in config_json.keys()]):
                raise ValueError("env key must be str.")
            if "mcpServers" in config_json:
                config_json = config_json["mcpServers"]
            # ç›´æ¥ä½¿ç”¨jsoné…ç½®é‡Œçš„id
            logging.info(f'ç”¨æˆ· {user_id} æ·»åŠ æ–°MCPæœåŠ¡å™¨: {config_json}')
            server_id = list(config_json.keys())[0]
            server_cmd = config_json[server_id]["command"]
            server_args = config_json[server_id]["args"]
            server_env = config_json[server_id].get('env')
        except Exception as e:
            status, msg = False, "The config must be a valid JSON."

    if not re.match(r'^[a-zA-Z][a-zA-Z0-9_]*$', server_id):
        status, msg = False, "The server id must be a valid variable name!"
    elif not server_cmd or server_cmd not in mcp_command_list:
        status, msg = False, "The server command is invalid!"
    
    if server_env:
        try:
            server_env = json.loads(server_env) if not isinstance(server_env, dict) else server_env
            if not all([isinstance(k, str) for k in server_env.keys()]):
                raise ValueError("env key must be str.")
            if not all([isinstance(v, str) for v in server_env.values()]):
                raise ValueError("env value must be str.")
        except Exception as e:
            server_env = {}
            status, msg = False, "The server env must be a JSON dict[str, str]."
    
    if isinstance(server_args, str):
        server_args = [x.strip() for x in server_args.split(' ') if x.strip()]

    logging.info(f'ç”¨æˆ· {user_id} æ·»åŠ æ–°MCPæœåŠ¡å™¨: {server_id}:{server_name}')
    
    if status:
        status, msg = request_add_mcp_server(user_id, server_id, server_name, server_cmd, 
                                         args=server_args, env=server_env, config_json=config_json)
    
    return status, msg

def chat_function(user_id, message, history, model_name, model_id_map, mcp_servers, selected_servers, 
                  system_prompt, max_tokens, budget_tokens, temperature, n_recent_images, enable_thinking, enable_stream):
    """å¤„ç†èŠå¤©åŠŸèƒ½"""
    # æ„å»ºæ¶ˆæ¯åˆ—è¡¨
    messages = [{"role": "system", "content": system_prompt}]
    
    # æ·»åŠ å†å²æ¶ˆæ¯
    for user_msg, bot_msg in history:
        messages.append({"role": "user", "content": user_msg})
        messages.append({"role": "assistant", "content": bot_msg})
    
    # æ·»åŠ å½“å‰ç”¨æˆ·æ¶ˆæ¯
    messages.append({"role": "user", "content": message})
    
    # è·å–é€‰ä¸­çš„MCPæœåŠ¡å™¨ID
    mcp_server_ids = [mcp_servers[server]["server_id"] for server in selected_servers]
    
    # è·å–æ¨¡å‹ID
    model_id = model_id_map[model_name]
    
    # æ„å»ºé¢å¤–å‚æ•°
    extra_params = {
        "only_n_most_recent_images": n_recent_images,
        "budget_tokens": budget_tokens,
        "enable_thinking": enable_thinking
    }
    
    # è¯·æ±‚èŠå¤©
    response, msg_extras = request_chat(
        user_id, messages, model_id, mcp_server_ids, 
        stream=enable_stream, max_tokens=max_tokens,
        temperature=temperature, extra_params=extra_params
    )
    
    full_response = ""
    thinking_content = ""
    tool_use_content = []
    
    # å¤„ç†æµå¼å“åº”
    if enable_stream and isinstance(response, requests.Response):
        for content in process_stream_response(response):
            full_response += content
            
            # å¤„ç†thinkingå†…å®¹
            thk_regex = r"<thinking>(.*?)</thinking>"
            thk_m = re.search(thk_regex, full_response, re.DOTALL)
            if thk_m:
                thinking_content = thk_m.group(1)
                full_response = re.sub(thk_regex, "", full_response, flags=re.DOTALL)
            
            # å¤„ç†tool_useå†…å®¹
            tooluse_regex = r"<tool_use>(.*?)</tool_use>"
            tool_m = re.search(tooluse_regex, full_response, re.DOTALL)
            if tool_m:
                tool_msg = tool_m.group(1)
                full_response = re.sub(tooluse_regex, "", full_response)
                tool_use_content.append(tool_msg)
            
            # æ›´æ–°UI
            yield full_response, thinking_content, json.dumps(tool_use_content, ensure_ascii=False, indent=2)
    else:
        # å¤„ç†éæµå¼å“åº”
        full_response = response if not isinstance(response, requests.Response) else "Error in response"
        
        # å¤„ç†thinkingå†…å®¹
        thk_regex = r"<thinking>(.*?)</thinking>"
        thk_m = re.search(thk_regex, full_response, re.DOTALL)
        if thk_m:
            thinking_content = thk_m.group(1)
            full_response = re.sub(thk_regex, "", full_response, flags=re.DOTALL)
        
        # å¤„ç†tool_useå†…å®¹
        if msg_extras.get('tool_use'):
            tool_use_content.append(json.dumps(msg_extras.get('tool_use')))
        
        yield full_response, thinking_content, json.dumps(tool_use_content, ensure_ascii=False, indent=2)

def refresh_mcp_servers(user_id):
    """åˆ·æ–°MCPæœåŠ¡å™¨åˆ—è¡¨"""
    mcp_servers = {}
    for server in request_list_mcp_servers(user_id):
        mcp_servers[server['server_name']] = {
            "server_id": server['server_id'],
            "server_desc": server.get('server_desc', server['server_name'])
        }
    return mcp_servers, list(mcp_servers.keys())

def refresh_models(user_id):
    """åˆ·æ–°æ¨¡å‹åˆ—è¡¨"""
    model_names = []
    model_id_map = {}
    models = request_list_models(user_id)
    logging.info(f"Retrieved models for refresh: {models}")
    
    # ç¡®ä¿modelsæ˜¯åˆ—è¡¨ç±»å‹
    if not isinstance(models, list):
        logging.warning(f"Models is not a list: {type(models)}")
        models = []
    
    for model in models:
        if isinstance(model, dict) and 'model_name' in model and 'model_id' in model:
            model_names.append(model['model_name'])
            model_id_map[model['model_name']] = model['model_id']
    
    # å¦‚æœæ²¡æœ‰è·å–åˆ°æ¨¡å‹ï¼Œä½¿ç”¨é»˜è®¤æ¨¡å‹
    if not model_names:
        logging.warning("No models retrieved, using fallback models")
        # æ·»åŠ ä¸€äº›é»˜è®¤æ¨¡å‹ä½œä¸ºå¤‡ç”¨
        fallback_models = [
            {"model_id": "us.anthropic.claude-3-5-sonnet-20241022-v2:0", "model_name": "Claude 3.5 Sonnet v2"},
            {"model_id": "us.amazon.nova-pro-v1:0", "model_name": "Amazon Nova Pro v1"}
        ]
        for model in fallback_models:
            model_names.append(model['model_name'])
            model_id_map[model['model_name']] = model['model_id']
    
    return model_names, model_id_map

def add_mcp_server_ui(user_id, server_name, server_id, server_cmd, server_args, server_env, server_config_json):
    """æ·»åŠ MCPæœåŠ¡å™¨UIå¤„ç†"""
    status, msg = add_new_mcp_server(
        user_id, server_name, server_id, server_cmd, 
        server_args, server_env, server_config_json
    )
    
    if status:
        # åˆ·æ–°æœåŠ¡å™¨åˆ—è¡¨
        mcp_servers, server_names = refresh_mcp_servers(user_id)
        return gr.update(value=""), gr.update(value=""), gr.update(value=""), gr.update(value=""), gr.update(value=""), gr.update(value=""), f"âœ… {msg}", mcp_servers, server_names
    else:
        return gr.update(), gr.update(), gr.update(), gr.update(), gr.update(), gr.update(), f"âŒ {msg}", gr.update(), gr.update()

def clear_conversation():
    """æ¸…ç©ºå¯¹è¯å†å²"""
    return [], "", "", ""

def generate_random_user_id():
    """ç”Ÿæˆéšæœºç”¨æˆ·ID"""
    return str(uuid.uuid4())[:8]

def save_user_id(user_id):
    """ä¿å­˜ç”¨æˆ·IDåˆ°cookie"""
    return user_id

def create_ui():
    """åˆ›å»ºGradio UI"""
    with gr.Blocks(title="ğŸ’¬ Customer Support Agent", css="""
        .container { max-width: 1200px; margin: auto; }
        .sidebar { min-width: 300px; }
        .chat-container { flex-grow: 1; }
        .tool-output { margin-top: 10px; }
    """) as demo:
        # åˆå§‹åŒ–ç”¨æˆ·ID
        user_id = gr.State(value=str(uuid.uuid4())[:8])
        
        # åˆå§‹åŒ–æ¨¡å‹å’ŒæœåŠ¡å™¨åˆ—è¡¨
        mcp_servers = gr.State({})
        model_id_map = gr.State({})
        
        gr.Markdown("# ğŸ’¬ Customer Support Agent")
        
        with gr.Row():
            with gr.Column(scale=3, elem_classes="chat-container"):
                chatbot = gr.Chatbot(height=600)
                
                with gr.Row():
                    msg = gr.Textbox(
                        placeholder="è¾“å…¥æ‚¨çš„æ¶ˆæ¯...",
                        show_label=False,
                        container=False,
                        scale=9
                    )
                    submit_btn = gr.Button("å‘é€", scale=1)
                
                with gr.Accordion("æ€è€ƒè¿‡ç¨‹", open=False):
                    thinking_output = gr.Textbox(label="Thinking", lines=10, interactive=False)
                
                with gr.Accordion("å·¥å…·ä½¿ç”¨", open=False):
                    tool_output = gr.Code(language="json", label="Tool Use", interactive=False)
                
                clear_btn = gr.Button("ğŸ—‘ï¸ æ¸…ç©ºå¯¹è¯")
            
            with gr.Column(scale=1, elem_classes="sidebar"):
                with gr.Group():
                    with gr.Row():
                        user_id_input = gr.Textbox(label="User ID", value=lambda: user_id.value)
                        refresh_id_btn = gr.Button("ğŸ”„", scale=1)
                
                model_dropdown = gr.Dropdown(label="æ¨¡å‹", interactive=True)
                
                max_tokens = gr.Slider(
                    minimum=1, maximum=64000, value=8000, step=1000,
                    label="æœ€å¤§è¾“å‡ºtoken"
                )
                
                budget_tokens = gr.Slider(
                    minimum=1024, maximum=128000, value=8192, step=1024,
                    label="æœ€å¤§æ€è€ƒtoken"
                )
                
                temperature = gr.Slider(
                    minimum=0.0, maximum=1.0, value=0.6, step=0.1,
                    label="Temperature"
                )
                
                n_recent_images = gr.Slider(
                    minimum=0, maximum=10, value=1, step=1,
                    label="æœ€è¿‘å›¾ç‰‡æ•°é‡"
                )
                
                system_prompt = gr.Textbox(
                    label="System Prompt",
                    value="""You are an advanced email customer service expert for LSCS, specializing in processing product inquiries and generating price quotes. Your primary functions include:

1. Extracting product codes and quantities from customer emails
2. Responding professionally to customer inquiries about product availability and pricing

Respond to customers in a helpful, professional manner while ensuring all pricing information is accurate and clearly presented.""",
                    lines=3
                )
                
                with gr.Row():
                    enable_thinking = gr.Checkbox(label="å¯ç”¨æ€è€ƒ", value=False)
                    enable_stream = gr.Checkbox(label="å¯ç”¨æµå¼è¾“å‡º", value=True)
                
                with gr.Accordion("MCP æœåŠ¡å™¨", open=True):
                    server_checkboxes = gr.CheckboxGroup(label="é€‰æ‹©æœåŠ¡å™¨")
                    
                    with gr.Accordion("æ·»åŠ æ–°æœåŠ¡å™¨", open=False):
                        with gr.Group():
                            new_server_name = gr.Textbox(label="æœåŠ¡å™¨åç§°", placeholder="Name description of server")
                            new_server_id = gr.Textbox(label="æœåŠ¡å™¨ID", placeholder="server id")
                            new_server_cmd = gr.Dropdown(label="è¿è¡Œå‘½ä»¤", choices=mcp_command_list)
                            new_server_args = gr.Textbox(label="è¿è¡Œå‚æ•°", placeholder="mcp-server-git --repository path/to/git/repo")
                            new_server_env = gr.Textbox(label="ç¯å¢ƒå˜é‡", placeholder="éœ€è¦æä¾›ä¸€ä¸ªæœ‰æ•ˆçš„JSONå­—å…¸")
                            new_server_config = gr.Textbox(label="JSONé…ç½®", placeholder="éœ€è¦æä¾›ä¸€ä¸ªæœ‰æ•ˆçš„JSONå­—å…¸", lines=5)
                            add_server_status = gr.Textbox(label="çŠ¶æ€", interactive=False)
                            add_server_btn = gr.Button("æ·»åŠ æœåŠ¡å™¨")
        
        # é¡µé¢åŠ è½½æ—¶åˆå§‹åŒ–æ•°æ®
        def init_data(request: gr.Request):
            current_user_id = get_user_id(request)
            logging.info(f"Initializing data for user: {current_user_id}")
            mcp_servers_data, server_names = refresh_mcp_servers(current_user_id)
            model_names, model_id_map_data = refresh_models(current_user_id)
            logging.info(f"Initialized with models: {model_names}")
            logging.info(f"Initialized with servers: {server_names}")
            
            return (
                current_user_id,
                mcp_servers_data,
                model_id_map_data,
                gr.update(choices=model_names, value=model_names[0] if model_names else None),
                gr.update(choices=server_names)
            )
        
        demo.load(
            init_data,
            inputs=[],
            outputs=[user_id, mcp_servers, model_id_map, model_dropdown, server_checkboxes]
        )
        
        # åˆ·æ–°ç”¨æˆ·ID
        refresh_id_btn.click(
            generate_random_user_id,
            outputs=[user_id_input]
        )
        
        # ä¿å­˜ç”¨æˆ·ID
        user_id_input.change(
            save_user_id,
            inputs=[user_id_input],
            outputs=[user_id]
        )
        
        # æ·»åŠ MCPæœåŠ¡å™¨
        add_server_btn.click(
            add_mcp_server_ui,
            inputs=[
                user_id, new_server_name, new_server_id, new_server_cmd,
                new_server_args, new_server_env, new_server_config
            ],
            outputs=[
                new_server_name, new_server_id, new_server_cmd,
                new_server_args, new_server_env, new_server_config,
                add_server_status, mcp_servers, server_checkboxes
            ]
        )
        
        # æ¸…ç©ºå¯¹è¯
        clear_btn.click(
            clear_conversation,
            outputs=[chatbot, msg, thinking_output, tool_output]
        )
        
        # å¤„ç†èŠå¤©
        chat_event = msg.submit(
            chat_function,
            inputs=[
                user_id, msg, chatbot, model_dropdown, model_id_map, 
                mcp_servers, server_checkboxes, system_prompt, max_tokens,
                budget_tokens, temperature, n_recent_images, 
                enable_thinking, enable_stream
            ],
            outputs=[thinking_output, tool_output],
            queue=True
        ).then(
            lambda x, y, z: ((y, x), ""),
            inputs=[msg, chatbot, thinking_output],
            outputs=[chatbot, msg]
        )
        
        submit_btn.click(
            chat_function,
            inputs=[
                user_id, msg, chatbot, model_dropdown, model_id_map, 
                mcp_servers, server_checkboxes, system_prompt, max_tokens,
                budget_tokens, temperature, n_recent_images, 
                enable_thinking, enable_stream
            ],
            outputs=[thinking_output, tool_output],
            queue=True
        ).then(
            lambda x, y, z: ((y, x), ""),
            inputs=[msg, chatbot, thinking_output],
            outputs=[chatbot, msg]
        )
        
    return demo

if __name__ == "__main__":
    port = int(os.environ.get("CHATBOT_SERVICE_PORT", 8502))
    demo = create_ui()
    demo.queue()
    demo.launch(server_name="0.0.0.0", server_port=port, share=False)
